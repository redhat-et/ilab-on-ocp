# PIPELINE DEFINITION
# Name: instructlab
# Description: InstructLab pipeline
# Inputs:
#    base_model: str [Default: 'ibm-granite/granite-7b-base']
#    batch_size: int [Default: 8.0]
#    device: str
#    few_shots: int [Default: 5.0]
#    max_workers: str [Default: 'auto']
#    merge_system_user_message: bool [Default: False]
#    mmlu_tasks_list: str [Default: 'mmlu_anatomy,mmlu_astronomy']
#    model_dtype: str [Default: 'bfloat16']
#    nnodes: int [Default: 2.0]
#    nproc_per_node: int [Default: 3.0]
#    num_instructions_to_generate: int [Default: 2.0]
#    repo_branch: str
#    repo_pr: int
#    repo_url: str [Default: 'https://github.com/instructlab/taxonomy.git']
#    storage_class_name: str [Default: 'nfs-csi']
components:
  comp-artifact-to-pvc-op:
    executorLabel: exec-artifact-to-pvc-op
    inputDefinitions:
      artifacts:
        data:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pvc_path:
          parameterType: STRING
  comp-artifact-to-pvc-op-2:
    executorLabel: exec-artifact-to-pvc-op-2
    inputDefinitions:
      artifacts:
        data:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pvc_path:
          parameterType: STRING
  comp-createpvc:
    executorLabel: exec-createpvc
    inputDefinitions:
      parameters:
        access_modes:
          description: 'AccessModes to request for the provisioned PVC. May

            be one or more of ``''ReadWriteOnce''``, ``''ReadOnlyMany''``, ``''ReadWriteMany''``,
            or

            ``''ReadWriteOncePod''``. Corresponds to `PersistentVolumeClaim.spec.accessModes
            <https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes>`_.'
          parameterType: LIST
        annotations:
          description: Annotations for the PVC's metadata. Corresponds to `PersistentVolumeClaim.metadata.annotations
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaim>`_.
          isOptional: true
          parameterType: STRUCT
        pvc_name:
          description: 'Name of the PVC. Corresponds to `PersistentVolumeClaim.metadata.name
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaim>`_.
            Only one of ``pvc_name`` and ``pvc_name_suffix`` can

            be provided.'
          isOptional: true
          parameterType: STRING
        pvc_name_suffix:
          description: 'Prefix to use for a dynamically generated name, which

            will take the form ``<argo-workflow-name>-<pvc_name_suffix>``. Only one

            of ``pvc_name`` and ``pvc_name_suffix`` can be provided.'
          isOptional: true
          parameterType: STRING
        size:
          description: The size of storage requested by the PVC that will be provisioned.
            For example, ``'5Gi'``. Corresponds to `PersistentVolumeClaim.spec.resources.requests.storage
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaimSpec>`_.
          parameterType: STRING
        storage_class_name:
          defaultValue: ''
          description: 'Name of StorageClass from which to provision the PV

            to back the PVC. ``None`` indicates to use the cluster''s default

            storage_class_name. Set to ``''''`` for a statically specified PVC.'
          isOptional: true
          parameterType: STRING
        volume_name:
          description: 'Pre-existing PersistentVolume that should back the

            provisioned PersistentVolumeClaim. Used for statically

            specified PV only. Corresponds to `PersistentVolumeClaim.spec.volumeName
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaimSpec>`_.'
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        name:
          parameterType: STRING
  comp-createpvc-2:
    executorLabel: exec-createpvc-2
    inputDefinitions:
      parameters:
        access_modes:
          description: 'AccessModes to request for the provisioned PVC. May

            be one or more of ``''ReadWriteOnce''``, ``''ReadOnlyMany''``, ``''ReadWriteMany''``,
            or

            ``''ReadWriteOncePod''``. Corresponds to `PersistentVolumeClaim.spec.accessModes
            <https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes>`_.'
          parameterType: LIST
        annotations:
          description: Annotations for the PVC's metadata. Corresponds to `PersistentVolumeClaim.metadata.annotations
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaim>`_.
          isOptional: true
          parameterType: STRUCT
        pvc_name:
          description: 'Name of the PVC. Corresponds to `PersistentVolumeClaim.metadata.name
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaim>`_.
            Only one of ``pvc_name`` and ``pvc_name_suffix`` can

            be provided.'
          isOptional: true
          parameterType: STRING
        pvc_name_suffix:
          description: 'Prefix to use for a dynamically generated name, which

            will take the form ``<argo-workflow-name>-<pvc_name_suffix>``. Only one

            of ``pvc_name`` and ``pvc_name_suffix`` can be provided.'
          isOptional: true
          parameterType: STRING
        size:
          description: The size of storage requested by the PVC that will be provisioned.
            For example, ``'5Gi'``. Corresponds to `PersistentVolumeClaim.spec.resources.requests.storage
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaimSpec>`_.
          parameterType: STRING
        storage_class_name:
          defaultValue: ''
          description: 'Name of StorageClass from which to provision the PV

            to back the PVC. ``None`` indicates to use the cluster''s default

            storage_class_name. Set to ``''''`` for a statically specified PVC.'
          isOptional: true
          parameterType: STRING
        volume_name:
          description: 'Pre-existing PersistentVolume that should back the

            provisioned PersistentVolumeClaim. Used for statically

            specified PV only. Corresponds to `PersistentVolumeClaim.spec.volumeName
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaimSpec>`_.'
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        name:
          parameterType: STRING
  comp-createpvc-3:
    executorLabel: exec-createpvc-3
    inputDefinitions:
      parameters:
        access_modes:
          description: 'AccessModes to request for the provisioned PVC. May

            be one or more of ``''ReadWriteOnce''``, ``''ReadOnlyMany''``, ``''ReadWriteMany''``,
            or

            ``''ReadWriteOncePod''``. Corresponds to `PersistentVolumeClaim.spec.accessModes
            <https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes>`_.'
          parameterType: LIST
        annotations:
          description: Annotations for the PVC's metadata. Corresponds to `PersistentVolumeClaim.metadata.annotations
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaim>`_.
          isOptional: true
          parameterType: STRUCT
        pvc_name:
          description: 'Name of the PVC. Corresponds to `PersistentVolumeClaim.metadata.name
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaim>`_.
            Only one of ``pvc_name`` and ``pvc_name_suffix`` can

            be provided.'
          isOptional: true
          parameterType: STRING
        pvc_name_suffix:
          description: 'Prefix to use for a dynamically generated name, which

            will take the form ``<argo-workflow-name>-<pvc_name_suffix>``. Only one

            of ``pvc_name`` and ``pvc_name_suffix`` can be provided.'
          isOptional: true
          parameterType: STRING
        size:
          description: The size of storage requested by the PVC that will be provisioned.
            For example, ``'5Gi'``. Corresponds to `PersistentVolumeClaim.spec.resources.requests.storage
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaimSpec>`_.
          parameterType: STRING
        storage_class_name:
          defaultValue: ''
          description: 'Name of StorageClass from which to provision the PV

            to back the PVC. ``None`` indicates to use the cluster''s default

            storage_class_name. Set to ``''''`` for a statically specified PVC.'
          isOptional: true
          parameterType: STRING
        volume_name:
          description: 'Pre-existing PersistentVolume that should back the

            provisioned PersistentVolumeClaim. Used for statically

            specified PV only. Corresponds to `PersistentVolumeClaim.spec.volumeName
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaimSpec>`_.'
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        name:
          parameterType: STRING
  comp-data-processing-op:
    executorLabel: exec-data-processing-op
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        sdg:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        max_batch_len:
          defaultValue: 20000.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        max_seq_len:
          defaultValue: 4096.0
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        processed_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-deletepvc:
    executorLabel: exec-deletepvc
    inputDefinitions:
      parameters:
        pvc_name:
          description: Name of the PVC to delete. Supports passing a runtime-generated
            name, such as a name provided by ``kubernetes.CreatePvcOp().outputs['name']``.
          parameterType: STRING
  comp-deletepvc-2:
    executorLabel: exec-deletepvc-2
    inputDefinitions:
      parameters:
        pvc_name:
          description: Name of the PVC to delete. Supports passing a runtime-generated
            name, such as a name provided by ``kubernetes.CreatePvcOp().outputs['name']``.
          parameterType: STRING
  comp-deletepvc-3:
    executorLabel: exec-deletepvc-3
    inputDefinitions:
      parameters:
        pvc_name:
          description: Name of the PVC to delete. Supports passing a runtime-generated
            name, such as a name provided by ``kubernetes.CreatePvcOp().outputs['name']``.
          parameterType: STRING
  comp-git-clone-op:
    executorLabel: exec-git-clone-op
    inputDefinitions:
      parameters:
        repo_branch:
          parameterType: STRING
        repo_pr:
          parameterType: NUMBER_INTEGER
        repo_url:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        taxonomy:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-huggingface-importer-op:
    executorLabel: exec-huggingface-importer-op
    inputDefinitions:
      parameters:
        repo_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-kubectl-apply-op:
    executorLabel: exec-kubectl-apply-op
    inputDefinitions:
      parameters:
        manifest:
          parameterType: STRING
  comp-kubectl-apply-op-2:
    executorLabel: exec-kubectl-apply-op-2
    inputDefinitions:
      parameters:
        manifest:
          parameterType: STRING
  comp-kubectl-wait-for-op:
    executorLabel: exec-kubectl-wait-for-op
    inputDefinitions:
      parameters:
        condition:
          parameterType: STRING
        kind:
          parameterType: STRING
        name:
          parameterType: STRING
  comp-kubectl-wait-for-op-2:
    executorLabel: exec-kubectl-wait-for-op-2
    inputDefinitions:
      parameters:
        condition:
          parameterType: STRING
        kind:
          parameterType: STRING
        name:
          parameterType: STRING
  comp-list-models-in-directory-op:
    executorLabel: exec-list-models-in-directory-op
    inputDefinitions:
      parameters:
        models_folder:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: LIST
  comp-list-models-in-directory-op-2:
    executorLabel: exec-list-models-in-directory-op-2
    inputDefinitions:
      parameters:
        models_folder:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: LIST
  comp-load-mmlu-results-op:
    executorLabel: exec-load-mmlu-results-op
    inputDefinitions:
      artifacts:
        mmlu_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
    outputDefinitions:
      parameters:
        Output:
          parameterType: LIST
  comp-pvc-to-artifact-op:
    executorLabel: exec-pvc-to-artifact-op
    inputDefinitions:
      parameters:
        pvc_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-pvc-to-model-op:
    executorLabel: exec-pvc-to-model-op
    inputDefinitions:
      parameters:
        pvc_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-pytorchjob-manifest-op:
    executorLabel: exec-pytorchjob-manifest-op
    inputDefinitions:
      parameters:
        input_pvc_name:
          parameterType: STRING
        model_pvc_name:
          parameterType: STRING
        name_suffix:
          parameterType: STRING
        nnodes:
          defaultValue: 2.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        nproc_per_node:
          defaultValue: 3.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        output_pvc_name:
          parameterType: STRING
        path_to_model:
          parameterType: STRING
        phase_name:
          parameterType: STRING
    outputDefinitions:
      parameters:
        manifest:
          parameterType: STRING
        name:
          parameterType: STRING
  comp-pytorchjob-manifest-op-2:
    executorLabel: exec-pytorchjob-manifest-op-2
    inputDefinitions:
      parameters:
        input_pvc_name:
          parameterType: STRING
        model_pvc_name:
          parameterType: STRING
        name_suffix:
          parameterType: STRING
        nnodes:
          defaultValue: 2.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        nproc_per_node:
          defaultValue: 3.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        output_pvc_name:
          parameterType: STRING
        path_to_model:
          parameterType: STRING
        phase_name:
          parameterType: STRING
    outputDefinitions:
      parameters:
        manifest:
          parameterType: STRING
        name:
          parameterType: STRING
  comp-run-mmlu-op:
    executorLabel: exec-run-mmlu-op
    inputDefinitions:
      parameters:
        batch_size:
          parameterType: NUMBER_INTEGER
        device:
          isOptional: true
          parameterType: STRING
        few_shots:
          parameterType: NUMBER_INTEGER
        mmlu_tasks_list:
          parameterType: STRING
        model_dtype:
          parameterType: STRING
        models_folder:
          isOptional: true
          parameterType: STRING
        models_list:
          isOptional: true
          parameterType: LIST
        models_path_prefix:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        mmlu_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        best_model:
          parameterType: STRING
        best_score:
          parameterType: NUMBER_DOUBLE
  comp-run-mt-bench-op:
    executorLabel: exec-run-mt-bench-op
    inputDefinitions:
      parameters:
        device:
          isOptional: true
          parameterType: STRING
        max_workers:
          parameterType: STRING
        merge_system_user_message:
          parameterType: BOOLEAN
        models_folder:
          isOptional: true
          parameterType: STRING
        models_list:
          isOptional: true
          parameterType: LIST
        models_path_prefix:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        mt_bench_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        best_model:
          parameterType: STRING
        best_score:
          parameterType: NUMBER_DOUBLE
  comp-sdg-op:
    executorLabel: exec-sdg-op
    inputDefinitions:
      artifacts:
        taxonomy:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        num_instructions_to_generate:
          parameterType: NUMBER_INTEGER
        repo_branch:
          parameterType: STRING
        repo_pr:
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        sdg:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-artifact-to-pvc-op:
      container:
        args:
        - cp -r {{$.inputs.artifacts['data'].path}} {{$.inputs.parameters['pvc_path']}}
        command:
        - /bin/sh
        - -c
        image: registry.access.redhat.com/ubi9/toolbox
    exec-artifact-to-pvc-op-2:
      container:
        args:
        - cp -r {{$.inputs.artifacts['data'].path}} {{$.inputs.parameters['pvc_path']}}
        command:
        - /bin/sh
        - -c
        image: registry.access.redhat.com/ubi9/toolbox
    exec-createpvc:
      container:
        image: argostub/createpvc
    exec-createpvc-2:
      container:
        image: argostub/createpvc
    exec-createpvc-3:
      container:
        image: argostub/createpvc
    exec-data-processing-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_processing_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'instructlab-training@git+https://github.com/instructlab/training.git'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_processing_op(\n    sdg: dsl.Input[dsl.Dataset],\n    processed_data:\
          \ dsl.Output[dsl.Dataset],\n    model: dsl.Input[dsl.Artifact],\n    max_seq_len:\
          \ Optional[int] = 4096,\n    max_batch_len: Optional[int] = 20000,\n):\n\
          \    import os\n\n    import instructlab.training.data_process as dp\n \
          \   from instructlab.training import (\n        DataProcessArgs,\n     \
          \   TrainingArgs,\n    )\n\n    # define training-specific arguments\n \
          \   training_args = TrainingArgs(\n        # define data-specific arguments\n\
          \        model_path=model.path,\n        data_path=f\"{sdg.path}/*_train_msgs*.jsonl\"\
          ,\n        data_output_dir=processed_data.path,\n        # define model-trianing\
          \ parameters\n        max_seq_len=max_seq_len,\n        max_batch_len=max_batch_len,\n\
          \        # XXX(shanand): We don't need the following arguments\n       \
          \ # for data processing. Added them for now to avoid\n        # Pydantic\
          \ validation errors for TrainingArgs\n        ckpt_output_dir=\"data/saved_checkpoints\"\
          ,\n        num_epochs=2,\n        effective_batch_size=3840,\n        save_samples=0,\n\
          \        learning_rate=2e-6,\n        warmup_steps=800,\n        is_padding_free=True,\n\
          \    )\n\n    def data_processing(train_args: TrainingArgs) -> None:\n \
          \       # early validation logic here\n        if train_args.max_batch_len\
          \ < train_args.max_seq_len:\n            raise ValueError(\n           \
          \     f\"the `max_batch_len` cannot be less than `max_seq_len`: {train_args.max_batch_len=}\
          \ < {train_args.max_seq_len=}\"\n            )\n\n            # process\
          \ the training data\n        if not os.path.exists(train_args.data_output_dir):\n\
          \            os.makedirs(train_args.data_output_dir, exist_ok=True)\n  \
          \      dp.main(\n            DataProcessArgs(\n                # XXX(osilkin):\
          \ make a decision here, either:\n                #   1. the CLI is fully\
          \ responsible for managing where the data is written\n                #\
          \   2. we never cache it and simply write it to a tmp file every time.\n\
          \                #\n                # An important reason for why #1 would\
          \ be preferable is in the case of OpenShift/SELinux\n                # where\
          \ the user has a defined place for new temporary data to be written.\n \
          \               data_output_path=train_args.data_output_dir,\n         \
          \       model_path=train_args.model_path,\n                data_path=train_args.data_path,\n\
          \                max_seq_len=train_args.max_seq_len,\n                chat_tmpl_path=train_args.chat_tmpl_path,\n\
          \            )\n        )\n\n    data_processing(train_args=training_args)\n\
          \n"
        image: registry.access.redhat.com/ubi9/python-311:latest
    exec-deletepvc:
      container:
        image: argostub/deletepvc
    exec-deletepvc-2:
      container:
        image: argostub/deletepvc
    exec-deletepvc-3:
      container:
        image: argostub/deletepvc
    exec-git-clone-op:
      container:
        args:
        - 'git clone {{$.inputs.parameters[''repo_url'']}} {{$.outputs.artifacts[''taxonomy''].path}}
          && cd {{$.outputs.artifacts[''taxonomy''].path}} && if [ -n "{{$.inputs.parameters[''repo_branch'']}}"
          ]; then git fetch origin {{$.inputs.parameters[''repo_branch'']}} && git
          checkout {{$.inputs.parameters[''repo_branch'']}}; elif [ -n "{{$.inputs.parameters[''repo_pr'']}}"
          ] && [ {{$.inputs.parameters[''repo_pr'']}} -gt 0 ]; then git fetch origin
          pull/{{$.inputs.parameters[''repo_pr'']}}/head:{{$.inputs.parameters[''repo_pr'']}}
          && git checkout {{$.inputs.parameters[''repo_pr'']}}; fi '
        command:
        - /bin/sh
        - -c
        image: registry.access.redhat.com/ubi9/toolbox
    exec-huggingface-importer-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - huggingface_importer_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'huggingface_hub'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef huggingface_importer_op(model: dsl.Output[dsl.Model], repo_name:\
          \ str):\n    from huggingface_hub import snapshot_download\n\n    snapshot_download(repo_id=repo_name,\
          \ cache_dir=\"/tmp\", local_dir=model.path)\n\n"
        image: registry.access.redhat.com/ubi9/python-311:latest
    exec-kubectl-apply-op:
      container:
        args:
        - echo "{{$.inputs.parameters['manifest']}}" | kubectl apply -f -
        command:
        - /bin/sh
        - -c
        image: registry.redhat.io/openshift4/ose-cli
    exec-kubectl-apply-op-2:
      container:
        args:
        - echo "{{$.inputs.parameters['manifest']}}" | kubectl apply -f -
        command:
        - /bin/sh
        - -c
        image: registry.redhat.io/openshift4/ose-cli
    exec-kubectl-wait-for-op:
      container:
        args:
        - kubectl wait --for={{$.inputs.parameters['condition']}} {{$.inputs.parameters['kind']}}/{{$.inputs.parameters['name']}}
          --timeout=2h
        command:
        - /bin/sh
        - -c
        image: registry.redhat.io/openshift4/ose-cli
    exec-kubectl-wait-for-op-2:
      container:
        args:
        - kubectl wait --for={{$.inputs.parameters['condition']}} {{$.inputs.parameters['kind']}}/{{$.inputs.parameters['name']}}
          --timeout=2h
        command:
        - /bin/sh
        - -c
        image: registry.redhat.io/openshift4/ose-cli
    exec-list-models-in-directory-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - list_models_in_directory_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef list_models_in_directory_op(models_folder: str) -> List:\n  \
          \  import os\n\n    models = os.listdir(models_folder)\n    return models\n\
          \n"
        image: python:3.8
    exec-list-models-in-directory-op-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - list_models_in_directory_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef list_models_in_directory_op(models_folder: str) -> List:\n  \
          \  import os\n\n    models = os.listdir(models_folder)\n    return models\n\
          \n"
        image: python:3.8
    exec-load-mmlu-results-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_mmlu_results_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_mmlu_results_op(mmlu_output: Input[Artifact]) -> list:\n\
          \    import json\n\n    mmlu_score_list = []\n    with open(mmlu_output.path,\
          \ \"r\") as f:\n        mmlu_score_list = json.load(f)\n\n    print(\"MMLU\
          \ Evaluation Data:\")\n    for mmlu_score in mmlu_score_list:\n        print(json.dumps(mmlu_score,\
          \ indent=4))\n\n    return mmlu_score_list\n\n"
        image: registry.access.redhat.com/ubi9/python-311:latest
    exec-pvc-to-artifact-op:
      container:
        args:
        - cp -r {{$.inputs.parameters['pvc_path']}} {{$.outputs.artifacts['model'].path}}
        command:
        - /bin/sh
        - -c
        image: registry.access.redhat.com/ubi9/toolbox
    exec-pvc-to-model-op:
      container:
        args:
        - cp -r {{$.inputs.parameters['pvc_path']}} {{$.outputs.artifacts['model'].path}}
        command:
        - /bin/sh
        - -c
        image: registry.access.redhat.com/ubi9/toolbox
    exec-pytorchjob-manifest-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - pytorchjob_manifest_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef pytorchjob_manifest_op(\n    model_pvc_name: str,\n    input_pvc_name:\
          \ str,\n    output_pvc_name: str,\n    name_suffix: str,\n    path_to_model:\
          \ str,\n    phase_name: str,\n    nproc_per_node: int = 3,\n    nnodes:\
          \ int = 2,\n) -> NamedTuple(\"outputs\", manifest=str, name=str):\n    import\
          \ inspect\n\n    Outputs = NamedTuple(\"outputs\", manifest=str, name=str)\n\
          \    name = f\"train-{phase_name}-{name_suffix.rstrip('-sdg')}\"\n\n   \
          \ image = \"quay.io/shanand/test-train:0.0.4\"\n\n    manifest = inspect.cleandoc(\n\
          \        f\"\"\"\n        apiVersion: kubeflow.org/v1\n        kind: PyTorchJob\n\
          \        metadata:\n          name: {name}\n        spec:\n          nprocPerNode:\
          \ \\\\\"{nproc_per_node}\\\\\"\n          pytorchReplicaSpecs:\n       \
          \     Master:\n              replicas: 1\n              restartPolicy: OnFailure\n\
          \              template:\n                metadata:\n                  annotations:\n\
          \                    sidecar.istio.io/inject: 'false'\n                spec:\n\
          \                  containers:\n                    - args:\n          \
          \              - |\n                          mkdir -p /output/model;\n\
          \                          mkdir -p /output/data;\n                    \
          \      python3.11 -u run_main_ds.py --model_path {path_to_model} --ckpt_output_dir\
          \ /output/model --data_output_dir /input_data/processed_data\n         \
          \             command:\n                        - /bin/bash\n          \
          \              - '-c'\n                        - '--'\n                \
          \      image: {image}\n                      name: pytorch\n           \
          \           volumeMounts:\n                        - mountPath: /input_data\n\
          \                          name: input-data\n                          readOnly:\
          \ true\n                        - mountPath: /input_model\n            \
          \              name: model\n                          readOnly: true\n \
          \                       - mountPath: /output\n                         \
          \ name: output\n                      env:\n                        - name:\
          \ NNODES\n                          value: \\\\\"{nnodes}\\\\\"\n      \
          \                  - name: NPROC_PER_NODE\n                          value:\
          \ \\\\\"{nproc_per_node}\\\\\"\n                      resources:\n     \
          \                   requests:\n                          cpu: 2\n      \
          \                    \"nvidia.com/gpu\": {nproc_per_node}\n            \
          \            limits:\n                          cpu: 2\n               \
          \           \"nvidia.com/gpu\": {nproc_per_node}\n                  volumes:\n\
          \                    - name: input-data\n                      persistentVolumeClaim:\n\
          \                        claimName: {input_pvc_name}\n                 \
          \   - name: model\n                      persistentVolumeClaim:\n      \
          \                  claimName: {model_pvc_name}\n                    - name:\
          \ output\n                      persistentVolumeClaim:\n               \
          \         claimName: {output_pvc_name}\n            Worker:\n          \
          \    replicas: {nnodes-1}\n              restartPolicy: OnFailure\n    \
          \          template:\n                metadata:\n                  annotations:\n\
          \                    sidecar.istio.io/inject: 'false'\n                spec:\n\
          \                  containers:\n                    - args:\n          \
          \              - |\n                          mkdir -p /tmp/model;\n   \
          \                       python3.11 -u run_main_ds.py --model_path {path_to_model}\
          \ --ckpt_output_dir /tmp/model --data_output_dir /input_data/processed_data\n\
          \                      command:\n                        - /bin/bash\n \
          \                       - '-c'\n                        - '--'\n       \
          \               image: {image}\n                      name: pytorch\n  \
          \                    volumeMounts:\n                        - mountPath:\
          \ /input_data\n                          name: input-data\n            \
          \              readOnly: true\n                        - mountPath: /input_model\n\
          \                          name: model\n                          readOnly:\
          \ true\n                        - mountPath: /output\n                 \
          \         name: output\n                          readOnly: true\n     \
          \                 env:\n                        - name: NNODES\n       \
          \                   value: \\\\\"{nnodes}\\\\\"\n                      \
          \  - name: NPROC_PER_NODE\n                          value: \\\\\"{nproc_per_node}\\\
          \\\"\n                      resources:\n                        requests:\n\
          \                          cpu: 2\n                          \"nvidia.com/gpu\"\
          : {nproc_per_node}\n                        limits:\n                  \
          \        cpu: 2\n                          \"nvidia.com/gpu\": {nproc_per_node}\n\
          \                  volumes:\n                    - name: input-data\n  \
          \                    persistentVolumeClaim:\n                        claimName:\
          \ {input_pvc_name}\n                    - name: model\n                \
          \      persistentVolumeClaim:\n                        claimName: {model_pvc_name}\n\
          \                    - name: output\n                      persistentVolumeClaim:\n\
          \                        claimName: {output_pvc_name}\n        \"\"\"\n\
          \    )\n\n    return Outputs(manifest, name)\n\n"
        image: registry.access.redhat.com/ubi9/python-311:latest
    exec-pytorchjob-manifest-op-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - pytorchjob_manifest_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef pytorchjob_manifest_op(\n    model_pvc_name: str,\n    input_pvc_name:\
          \ str,\n    output_pvc_name: str,\n    name_suffix: str,\n    path_to_model:\
          \ str,\n    phase_name: str,\n    nproc_per_node: int = 3,\n    nnodes:\
          \ int = 2,\n) -> NamedTuple(\"outputs\", manifest=str, name=str):\n    import\
          \ inspect\n\n    Outputs = NamedTuple(\"outputs\", manifest=str, name=str)\n\
          \    name = f\"train-{phase_name}-{name_suffix.rstrip('-sdg')}\"\n\n   \
          \ image = \"quay.io/shanand/test-train:0.0.4\"\n\n    manifest = inspect.cleandoc(\n\
          \        f\"\"\"\n        apiVersion: kubeflow.org/v1\n        kind: PyTorchJob\n\
          \        metadata:\n          name: {name}\n        spec:\n          nprocPerNode:\
          \ \\\\\"{nproc_per_node}\\\\\"\n          pytorchReplicaSpecs:\n       \
          \     Master:\n              replicas: 1\n              restartPolicy: OnFailure\n\
          \              template:\n                metadata:\n                  annotations:\n\
          \                    sidecar.istio.io/inject: 'false'\n                spec:\n\
          \                  containers:\n                    - args:\n          \
          \              - |\n                          mkdir -p /output/model;\n\
          \                          mkdir -p /output/data;\n                    \
          \      python3.11 -u run_main_ds.py --model_path {path_to_model} --ckpt_output_dir\
          \ /output/model --data_output_dir /input_data/processed_data\n         \
          \             command:\n                        - /bin/bash\n          \
          \              - '-c'\n                        - '--'\n                \
          \      image: {image}\n                      name: pytorch\n           \
          \           volumeMounts:\n                        - mountPath: /input_data\n\
          \                          name: input-data\n                          readOnly:\
          \ true\n                        - mountPath: /input_model\n            \
          \              name: model\n                          readOnly: true\n \
          \                       - mountPath: /output\n                         \
          \ name: output\n                      env:\n                        - name:\
          \ NNODES\n                          value: \\\\\"{nnodes}\\\\\"\n      \
          \                  - name: NPROC_PER_NODE\n                          value:\
          \ \\\\\"{nproc_per_node}\\\\\"\n                      resources:\n     \
          \                   requests:\n                          cpu: 2\n      \
          \                    \"nvidia.com/gpu\": {nproc_per_node}\n            \
          \            limits:\n                          cpu: 2\n               \
          \           \"nvidia.com/gpu\": {nproc_per_node}\n                  volumes:\n\
          \                    - name: input-data\n                      persistentVolumeClaim:\n\
          \                        claimName: {input_pvc_name}\n                 \
          \   - name: model\n                      persistentVolumeClaim:\n      \
          \                  claimName: {model_pvc_name}\n                    - name:\
          \ output\n                      persistentVolumeClaim:\n               \
          \         claimName: {output_pvc_name}\n            Worker:\n          \
          \    replicas: {nnodes-1}\n              restartPolicy: OnFailure\n    \
          \          template:\n                metadata:\n                  annotations:\n\
          \                    sidecar.istio.io/inject: 'false'\n                spec:\n\
          \                  containers:\n                    - args:\n          \
          \              - |\n                          mkdir -p /tmp/model;\n   \
          \                       python3.11 -u run_main_ds.py --model_path {path_to_model}\
          \ --ckpt_output_dir /tmp/model --data_output_dir /input_data/processed_data\n\
          \                      command:\n                        - /bin/bash\n \
          \                       - '-c'\n                        - '--'\n       \
          \               image: {image}\n                      name: pytorch\n  \
          \                    volumeMounts:\n                        - mountPath:\
          \ /input_data\n                          name: input-data\n            \
          \              readOnly: true\n                        - mountPath: /input_model\n\
          \                          name: model\n                          readOnly:\
          \ true\n                        - mountPath: /output\n                 \
          \         name: output\n                          readOnly: true\n     \
          \                 env:\n                        - name: NNODES\n       \
          \                   value: \\\\\"{nnodes}\\\\\"\n                      \
          \  - name: NPROC_PER_NODE\n                          value: \\\\\"{nproc_per_node}\\\
          \\\"\n                      resources:\n                        requests:\n\
          \                          cpu: 2\n                          \"nvidia.com/gpu\"\
          : {nproc_per_node}\n                        limits:\n                  \
          \        cpu: 2\n                          \"nvidia.com/gpu\": {nproc_per_node}\n\
          \                  volumes:\n                    - name: input-data\n  \
          \                    persistentVolumeClaim:\n                        claimName:\
          \ {input_pvc_name}\n                    - name: model\n                \
          \      persistentVolumeClaim:\n                        claimName: {model_pvc_name}\n\
          \                    - name: output\n                      persistentVolumeClaim:\n\
          \                        claimName: {output_pvc_name}\n        \"\"\"\n\
          \    )\n\n    return Outputs(manifest, name)\n\n"
        image: registry.access.redhat.com/ubi9/python-311:latest
    exec-run-mmlu-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - run_mmlu_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef run_mmlu_op(\n    mmlu_output: Output[Artifact],\n    models_path_prefix:\
          \ str,\n    mmlu_tasks_list: str,\n    model_dtype: str,\n    few_shots:\
          \ int,\n    batch_size: int,\n    device: str = None,\n    models_list:\
          \ List[str] = None,\n    models_folder: Optional[str] = None,\n) -> NamedTuple(\"\
          outputs\", best_model=str, best_score=float):\n    import json\n    import\
          \ os\n\n    import torch\n    from instructlab.eval.mmlu import MMLU_TASKS,\
          \ MMLUEvaluator\n\n    mmlu_tasks = mmlu_tasks_list.split(\",\") if mmlu_tasks_list\
          \ else MMLU_TASKS\n\n    if models_list is None and models_folder:\n   \
          \     models_list = os.listdir(models_folder)\n\n    # Device setup and\
          \ debug\n    gpu_available = torch.cuda.is_available()\n    gpu_name = (\n\
          \        torch.cuda.get_device_name(torch.cuda.current_device())\n     \
          \   if gpu_available\n        else \"No GPU available\"\n    )\n\n    print(f\"\
          GPU Available: {gpu_available}, Using: {gpu_name}\")\n\n    effective_device\
          \ = (\n        device if device is not None else (\"cuda\" if gpu_available\
          \ else \"cpu\")\n    )\n    print(f\"Running on device: {effective_device}\"\
          )\n\n    scores = {}\n    all_mmlu_data = []\n\n    for model_name in models_list:\n\
          \        model_path = f\"{models_path_prefix}/{model_name}\"\n        #\
          \ Debug\n        print(f\"Model {model_name} is stored at: {model_path}\"\
          )\n\n        # Evaluation\n        evaluator = MMLUEvaluator(\n        \
          \    model_path=model_path,\n            tasks=mmlu_tasks,\n           \
          \ model_dtype=model_dtype,\n            few_shots=few_shots,\n         \
          \   batch_size=batch_size,\n            device=effective_device,\n     \
          \   )\n\n        mmlu_score, individual_scores = evaluator.run()\n     \
          \   average_score = round(mmlu_score, 2)\n        print(\n            f\"\
          Model {model_name} is stored at: {model_path} with AVERAGE_SCORE: {average_score}\"\
          \n        )\n\n        mmlu_data = {\n            \"report_title\": \"KNOWLEDGE\
          \ EVALUATION REPORT\",\n            \"model\": model_name,\n           \
          \ \"average_score\": average_score,\n            \"number_of_tasks\": len(individual_scores),\n\
          \            \"individual_scores\": [\n                {task: round(score[\"\
          score\"], 2)}\n                for task, score in individual_scores.items()\n\
          \            ],\n        }\n\n        all_mmlu_data.append(mmlu_data)\n\
          \        scores[model_path] = average_score\n\n    with open(mmlu_output.path,\
          \ \"w\") as f:\n        json.dump(all_mmlu_data, f, indent=4)\n    outputs\
          \ = NamedTuple(\"outputs\", best_model=str, best_score=float)\n    best_model\
          \ = max(scores, key=scores.get)\n    best_score = scores[best_model]\n \
          \   return outputs(best_model=best_model, best_score=best_score)\n\n"
        image: quay.io/sallyom/instructlab-ocp:eval
        resources:
          accelerator:
            count: '1'
            type: nvidia.com/gpu
    exec-run-mt-bench-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - run_mt_bench_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'vllm' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef run_mt_bench_op(\n    models_path_prefix: str,\n    mt_bench_output:\
          \ Output[Artifact],\n    merge_system_user_message: bool,\n    # generate_answers,judgment\
          \ uses a magic word for its mt_bench evaluator  - `auto`\n    # with `auto`,\
          \ number of gpus allocated for serving is calculated based on environment\n\
          \    # https://github.com/instructlab/eval/blob/main/src/instructlab/eval/mt_bench.py#L36\n\
          \    max_workers: str,\n    models_list: List[str] = None,\n    models_folder:\
          \ Optional[str] = None,\n    device: str = None,\n) -> NamedTuple(\"outputs\"\
          , best_model=str, best_score=float):\n    def launch_vllm(model_path: str,\
          \ gpu_count: int, retries: int = 60, delay: int = 5):\n        import subprocess\n\
          \        import sys\n        import time\n\n        import requests\n\n\
          \        if gpu_count > 0:\n            command = [\n                sys.executable,\n\
          \                \"-m\",\n                \"vllm.entrypoints.openai.api_server\"\
          ,\n                \"--model\",\n                model_path,\n         \
          \       \"--tensor-parallel-size\",\n                str(gpu_count),\n \
          \           ]\n        else:\n            command = [\n                sys.executable,\n\
          \                \"-m\",\n                \"vllm.entrypoints.openai.api_server\"\
          ,\n                \"--model\",\n                model_path,\n         \
          \   ]\n\n        subprocess.Popen(args=command)\n\n        server_url =\
          \ \"http://localhost:8000/v1\"\n        print(f\"Waiting for vLLM server\
          \ to start at {server_url}...\")\n\n        for attempt in range(retries):\n\
          \            try:\n                response = requests.get(f\"{server_url}/models\"\
          )\n                if response.status_code == 200:\n                   \
          \ print(f\"vLLM server is up and running at {server_url}.\")\n         \
          \           return\n            except requests.ConnectionError:\n     \
          \           pass\n\n            print(\n                f\"Server not available\
          \ yet, retrying in {delay} seconds (Attempt {attempt + 1}/{retries})...\"\
          \n            )\n            time.sleep(delay)\n\n        raise RuntimeError(\n\
          \            f\"Failed to start vLLM server at {server_url} after {retries}\
          \ retries.\"\n        )\n\n    # This seems like excessive effort to stop\
          \ the vllm process, but merely saving & killing the pid doesn't work\n \
          \   # Also, the base image does not include `pkill` cmd, so can't pkill\
          \ -f vllm.entrypoints.openai.api_server either\n    def stop_vllm_server_by_name():\n\
          \        import psutil\n\n        for process in psutil.process_iter(attrs=[\"\
          pid\", \"name\", \"cmdline\"]):\n            cmdline = process.info.get(\"\
          cmdline\")\n            if cmdline and \"vllm.entrypoints.openai.api_server\"\
          \ in cmdline:\n                print(\n                    f\"Found vLLM\
          \ server process with PID: {process.info['pid']}, terminating...\"\n   \
          \             )\n                try:\n                    process.terminate()\
          \  # Try graceful termination\n                    process.wait(timeout=5)\
          \  # Wait a bit for it to terminate\n                    if process.is_running():\n\
          \                        print(\n                            f\"Forcefully\
          \ killing vLLM server process with PID: {process.info['pid']}\"\n      \
          \                  )\n                        process.kill()  # Force kill\
          \ if it's still running\n                    print(\n                  \
          \      f\"Successfully stopped vLLM server with PID: {process.info['pid']}\"\
          \n                    )\n                except psutil.NoSuchProcess:\n\
          \                    print(f\"Process with PID {process.info['pid']} no\
          \ longer exists.\")\n                except psutil.AccessDenied:\n     \
          \               print(\n                        f\"Access denied when trying\
          \ to terminate process with PID {process.info['pid']}.\"\n             \
          \       )\n                except Exception as e:\n                    print(\n\
          \                        f\"Failed to terminate process with PID {process.info['pid']}.\
          \ Error: {e}\"\n                    )\n\n    import json\n    import os\n\
          \n    import torch\n    from instructlab.eval.mt_bench import MTBenchEvaluator\n\
          \n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\
          \n    vllm_server = \"http://localhost:8000/v1\"\n\n    gpu_available =\
          \ torch.cuda.is_available()\n    gpu_name = (\n        torch.cuda.get_device_name(torch.cuda.current_device())\n\
          \        if gpu_available\n        else \"No GPU available\"\n    )\n  \
          \  gpu_count = torch.cuda.device_count() if gpu_available else 0\n\n   \
          \ print(f\"GPU Available: {gpu_available}, {gpu_name}\")\n\n    if models_list\
          \ is None and models_folder:\n        models_list = os.listdir(models_folder)\n\
          \n    judge_api_key = os.getenv(\"JUDGE_API_KEY\", \"\")\n    judge_model_name\
          \ = os.getenv(\"JUDGE_NAME\")\n    judge_endpoint = os.getenv(\"JUDGE_ENDPOINT\"\
          )\n\n    scores = {}\n    all_mt_bench_data = []\n\n    # generate_answers,judgment\
          \ uses a magic word for its mt_bench evaluator  - `auto`\n    # with `auto`,\
          \ number of gpus allocated for serving is calculated based on environment\n\
          \    # https://github.com/instructlab/eval/blob/main/src/instructlab/eval/mt_bench.py#L36\n\
          \    if max_workers == \"auto\":\n        try:\n            usable_cpu_count\
          \ = len(os.sched_getaffinity(0)) // 2\n        except AttributeError:\n\
          \            usable_cpu_count = multiprocessing.cpu_count() // 2\n     \
          \   max_workers = usable_cpu_count\n\n    for model_name in models_list:\n\
          \        print(f\"Serving candidate model: {model_name}\")\n        model_path\
          \ = f\"{models_path_prefix}/{model_name}\"\n\n        launch_vllm(model_path,\
          \ gpu_count)\n\n        # model ID is the model_path value in vLLM\n   \
          \     evaluator = MTBenchEvaluator(\n            model_name=model_path,\n\
          \            judge_model_name=judge_model_name,\n            output_dir=\"\
          /tmp/eval_output\",\n            merge_system_user_message=merge_system_user_message,\n\
          \        )\n\n        evaluator.gen_answers(\n            server_url=vllm_server,\n\
          \            serving_gpus=gpu_count,\n            max_workers=max_workers,\n\
          \        )\n\n        stop_vllm_server_by_name()\n\n        overall_score,\
          \ qa_pairs, turn_scores, error_rate = evaluator.judge_answers(\n       \
          \     server_url=judge_endpoint,\n            api_key=judge_api_key,\n \
          \           serving_gpus=gpu_count,\n            max_workers=max_workers,\n\
          \        )\n\n        mt_bench_data = {\n            \"report_title\": \"\
          SKILLS EVALUATION REPORT\",\n            \"model\": model_path,\n      \
          \      \"judge_model\": judge_model_name,\n            \"overall_score\"\
          : overall_score,\n            \"turn_scores\": turn_scores,\n          \
          \  \"qa_scores\": qa_pairs,\n            \"error_rate\": error_rate,\n \
          \       }\n\n        all_mt_bench_data.append(mt_bench_data)\n        scores[model_path]\
          \ = overall_score\n\n    with open(mt_bench_output.path, \"w\") as f:\n\
          \        json.dump(all_mt_bench_data, f, indent=4)\n\n    outputs = NamedTuple(\"\
          outputs\", best_model=str, best_score=float)\n    best_model = max(scores,\
          \ key=scores.get)\n    best_score = scores[best_model]\n    return outputs(best_model=best_model,\
          \ best_score=best_score)\n\n"
        image: quay.io/sallyom/instructlab-ocp:eval-7ee213
        resources:
          accelerator:
            count: '1'
            type: nvidia.com/gpu
    exec-sdg-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - sdg_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef sdg_op(\n    num_instructions_to_generate: int,\n    taxonomy:\
          \ dsl.Input[dsl.Dataset],\n    sdg: dsl.Output[dsl.Dataset],\n    repo_branch:\
          \ Optional[str],\n    repo_pr: Optional[int],\n):\n    from os import getenv\n\
          \n    import openai\n    from instructlab.sdg import generate_data\n   \
          \ from instructlab.sdg.utils.taxonomy import read_taxonomy\n\n    api_key\
          \ = getenv(\"api_key\")\n    model = getenv(\"model\")\n    endpoint = getenv(\"\
          endpoint\")\n    client = openai.OpenAI(base_url=endpoint, api_key=api_key)\n\
          \n    taxonomy_base = \"main\" if repo_branch or (repo_pr and int(repo_pr)\
          \ > 0) else \"empty\"\n\n    print(\"Generating syntetic dataset for:\"\
          )\n    print()\n    print(read_taxonomy(taxonomy.path, taxonomy_base))\n\
          \n    # generate_data has a magic word for its taxonomy_base argument -\
          \ `empty`\n    # it allows generating from the whole repo, see:\n    # https://github.com/instructlab/sdg/blob/c6a9e74a1618b1077cd38e713b8aaed8b7c0c8ce/src/instructlab/sdg/utils/taxonomy.py#L230\n\
          \    generate_data(\n        client=client,\n        num_instructions_to_generate=num_instructions_to_generate,\n\
          \        output_dir=sdg.path,\n        taxonomy=taxonomy.path,\n       \
          \ taxonomy_base=taxonomy_base,\n        model_name=model,\n        chunk_word_count=1000,\n\
          \        server_ctx_size=4096,\n    )\n\n"
        image: quay.io/tcoufal/ilab-sdg:latest
pipelineInfo:
  description: InstructLab pipeline
  displayName: InstructLab
  name: instructlab
root:
  dag:
    tasks:
      artifact-to-pvc-op:
        cachingOptions: {}
        componentRef:
          name: comp-artifact-to-pvc-op
        dependentTasks:
        - createpvc
        - huggingface-importer-op
        inputs:
          artifacts:
            data:
              taskOutputArtifact:
                outputArtifactKey: model
                producerTask: huggingface-importer-op
          parameters:
            pvc_path:
              runtimeValue:
                constant: /model
        retryPolicy:
          backoffDuration: 0s
          backoffFactor: 2.0
          backoffMaxDuration: 3600s
          maxRetryCount: 3
        taskInfo:
          name: artifact-to-pvc-op
      artifact-to-pvc-op-2:
        cachingOptions: {}
        componentRef:
          name: comp-artifact-to-pvc-op-2
        dependentTasks:
        - createpvc-2
        - data-processing-op
        inputs:
          artifacts:
            data:
              taskOutputArtifact:
                outputArtifactKey: processed_data
                producerTask: data-processing-op
          parameters:
            pvc_path:
              runtimeValue:
                constant: /data
        taskInfo:
          name: artifact-to-pvc-op-2
      createpvc:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-createpvc
        inputs:
          parameters:
            access_modes:
              runtimeValue:
                constant:
                - ReadWriteMany
            pvc_name_suffix:
              runtimeValue:
                constant: -model-cache
            size:
              runtimeValue:
                constant: 100Gi
            storage_class_name:
              componentInputParameter: storage_class_name
        taskInfo:
          name: createpvc
      createpvc-2:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-createpvc-2
        inputs:
          parameters:
            access_modes:
              runtimeValue:
                constant:
                - ReadWriteMany
            pvc_name_suffix:
              runtimeValue:
                constant: -sdg
            size:
              runtimeValue:
                constant: 1Gi
            storage_class_name:
              componentInputParameter: storage_class_name
        taskInfo:
          name: createpvc-2
      createpvc-3:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-createpvc-3
        inputs:
          parameters:
            access_modes:
              runtimeValue:
                constant:
                - ReadWriteMany
            pvc_name_suffix:
              runtimeValue:
                constant: -output
            size:
              runtimeValue:
                constant: 100Gi
            storage_class_name:
              componentInputParameter: storage_class_name
        taskInfo:
          name: createpvc-3
      data-processing-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-processing-op
        dependentTasks:
        - huggingface-importer-op
        - sdg-op
        inputs:
          artifacts:
            model:
              taskOutputArtifact:
                outputArtifactKey: model
                producerTask: huggingface-importer-op
            sdg:
              taskOutputArtifact:
                outputArtifactKey: sdg
                producerTask: sdg-op
        taskInfo:
          name: data-processing-op
      deletepvc:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-deletepvc
        dependentTasks:
        - createpvc-3
        - pvc-to-model-op
        inputs:
          parameters:
            pvc_name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc-3
        taskInfo:
          name: deletepvc
      deletepvc-2:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-deletepvc-2
        dependentTasks:
        - createpvc-2
        - pvc-to-model-op
        inputs:
          parameters:
            pvc_name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc-2
        taskInfo:
          name: deletepvc-2
      deletepvc-3:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-deletepvc-3
        dependentTasks:
        - createpvc
        - pvc-to-model-op
        inputs:
          parameters:
            pvc_name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc
        taskInfo:
          name: deletepvc-3
      git-clone-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-git-clone-op
        inputs:
          parameters:
            repo_branch:
              componentInputParameter: repo_branch
            repo_pr:
              componentInputParameter: repo_pr
            repo_url:
              componentInputParameter: repo_url
        taskInfo:
          name: git-clone-op
      huggingface-importer-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-huggingface-importer-op
        inputs:
          parameters:
            repo_name:
              componentInputParameter: base_model
        taskInfo:
          name: huggingface-importer-op
      kubectl-apply-op:
        cachingOptions: {}
        componentRef:
          name: comp-kubectl-apply-op
        dependentTasks:
        - artifact-to-pvc-op
        - artifact-to-pvc-op-2
        - pytorchjob-manifest-op
        inputs:
          parameters:
            manifest:
              taskOutputParameter:
                outputParameterKey: manifest
                producerTask: pytorchjob-manifest-op
        taskInfo:
          name: kubectl-apply-op
      kubectl-apply-op-2:
        cachingOptions: {}
        componentRef:
          name: comp-kubectl-apply-op-2
        dependentTasks:
        - artifact-to-pvc-op
        - artifact-to-pvc-op-2
        - pytorchjob-manifest-op-2
        inputs:
          parameters:
            manifest:
              taskOutputParameter:
                outputParameterKey: manifest
                producerTask: pytorchjob-manifest-op-2
        taskInfo:
          name: kubectl-apply-op-2
      kubectl-wait-for-op:
        cachingOptions: {}
        componentRef:
          name: comp-kubectl-wait-for-op
        dependentTasks:
        - kubectl-apply-op
        - pytorchjob-manifest-op
        inputs:
          parameters:
            condition:
              runtimeValue:
                constant: condition=Succeeded
            kind:
              runtimeValue:
                constant: pytorchjobs
            name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: pytorchjob-manifest-op
        taskInfo:
          name: kubectl-wait-for-op
      kubectl-wait-for-op-2:
        cachingOptions: {}
        componentRef:
          name: comp-kubectl-wait-for-op-2
        dependentTasks:
        - kubectl-apply-op-2
        - pytorchjob-manifest-op-2
        inputs:
          parameters:
            condition:
              runtimeValue:
                constant: condition=Succeeded
            kind:
              runtimeValue:
                constant: pytorchjobs
            name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: pytorchjob-manifest-op-2
        taskInfo:
          name: kubectl-wait-for-op-2
      list-models-in-directory-op:
        cachingOptions: {}
        componentRef:
          name: comp-list-models-in-directory-op
        dependentTasks:
        - createpvc-3
        - kubectl-wait-for-op
        inputs:
          parameters:
            models_folder:
              runtimeValue:
                constant: /output/model/model/hf_format
        taskInfo:
          name: list-models-in-directory-op
      list-models-in-directory-op-2:
        cachingOptions: {}
        componentRef:
          name: comp-list-models-in-directory-op-2
        dependentTasks:
        - createpvc-3
        - kubectl-wait-for-op-2
        inputs:
          parameters:
            models_folder:
              runtimeValue:
                constant: /output/model/model/hf_format
        taskInfo:
          name: list-models-in-directory-op-2
      load-mmlu-results-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-load-mmlu-results-op
        dependentTasks:
        - run-mmlu-op
        inputs:
          artifacts:
            mmlu_output:
              taskOutputArtifact:
                outputArtifactKey: mmlu_output
                producerTask: run-mmlu-op
        taskInfo:
          name: load-mmlu-results-op
      pvc-to-artifact-op:
        cachingOptions: {}
        componentRef:
          name: comp-pvc-to-artifact-op
        dependentTasks:
        - createpvc-3
        - run-mt-bench-op
        inputs:
          parameters:
            pvc_path:
              runtimeValue:
                constant: /output/data
        taskInfo:
          name: pvc-to-artifact-op
      pvc-to-model-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-pvc-to-model-op
        dependentTasks:
        - createpvc-3
        - run-mt-bench-op
        inputs:
          parameters:
            pvc_path:
              runtimeValue:
                constant: /output/model
        taskInfo:
          name: pvc-to-model-op
      pytorchjob-manifest-op:
        cachingOptions: {}
        componentRef:
          name: comp-pytorchjob-manifest-op
        dependentTasks:
        - createpvc
        - createpvc-2
        - createpvc-3
        inputs:
          parameters:
            input_pvc_name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc-2
            model_pvc_name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc
            name_suffix:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc-2
            nnodes:
              componentInputParameter: nnodes
            nproc_per_node:
              componentInputParameter: nproc_per_node
            output_pvc_name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc-3
            path_to_model:
              runtimeValue:
                constant: /input_model/model
            phase_name:
              runtimeValue:
                constant: first
        taskInfo:
          name: pytorchjob-manifest-op
      pytorchjob-manifest-op-2:
        cachingOptions: {}
        componentRef:
          name: comp-pytorchjob-manifest-op-2
        dependentTasks:
        - createpvc
        - createpvc-2
        - createpvc-3
        - run-mmlu-op
        inputs:
          parameters:
            input_pvc_name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc-2
            model_pvc_name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc
            name_suffix:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc-2
            nnodes:
              componentInputParameter: nnodes
            nproc_per_node:
              componentInputParameter: nproc_per_node
            output_pvc_name:
              taskOutputParameter:
                outputParameterKey: name
                producerTask: createpvc-3
            path_to_model:
              taskOutputParameter:
                outputParameterKey: best_model
                producerTask: run-mmlu-op
            phase_name:
              runtimeValue:
                constant: second
        taskInfo:
          name: pytorchjob-manifest-op-2
      run-mmlu-op:
        cachingOptions: {}
        componentRef:
          name: comp-run-mmlu-op
        dependentTasks:
        - createpvc-3
        - list-models-in-directory-op
        inputs:
          parameters:
            batch_size:
              componentInputParameter: batch_size
            device:
              componentInputParameter: device
            few_shots:
              componentInputParameter: few_shots
            mmlu_tasks_list:
              componentInputParameter: mmlu_tasks_list
            model_dtype:
              componentInputParameter: model_dtype
            models_list:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: list-models-in-directory-op
            models_path_prefix:
              runtimeValue:
                constant: /output/model/hf_format
        taskInfo:
          name: run-mmlu-op
      run-mt-bench-op:
        cachingOptions: {}
        componentRef:
          name: comp-run-mt-bench-op
        dependentTasks:
        - createpvc-3
        - list-models-in-directory-op-2
        inputs:
          parameters:
            device:
              componentInputParameter: device
            max_workers:
              componentInputParameter: max_workers
            merge_system_user_message:
              componentInputParameter: merge_system_user_message
            models_list:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: list-models-in-directory-op-2
            models_path_prefix:
              runtimeValue:
                constant: /output/model/hf_format
        taskInfo:
          name: run-mt-bench-op
      sdg-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-sdg-op
        dependentTasks:
        - git-clone-op
        inputs:
          artifacts:
            taxonomy:
              taskOutputArtifact:
                outputArtifactKey: taxonomy
                producerTask: git-clone-op
          parameters:
            num_instructions_to_generate:
              componentInputParameter: num_instructions_to_generate
            repo_branch:
              componentInputParameter: repo_branch
            repo_pr:
              componentInputParameter: repo_pr
        taskInfo:
          name: sdg-op
  inputDefinitions:
    parameters:
      base_model:
        defaultValue: ibm-granite/granite-7b-base
        isOptional: true
        parameterType: STRING
      batch_size:
        defaultValue: 8.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      device:
        isOptional: true
        parameterType: STRING
      few_shots:
        defaultValue: 5.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      max_workers:
        defaultValue: auto
        isOptional: true
        parameterType: STRING
      merge_system_user_message:
        defaultValue: false
        isOptional: true
        parameterType: BOOLEAN
      mmlu_tasks_list:
        defaultValue: mmlu_anatomy,mmlu_astronomy
        isOptional: true
        parameterType: STRING
      model_dtype:
        defaultValue: bfloat16
        isOptional: true
        parameterType: STRING
      nnodes:
        defaultValue: 2.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      nproc_per_node:
        defaultValue: 3.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      num_instructions_to_generate:
        defaultValue: 2.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      repo_branch:
        isOptional: true
        parameterType: STRING
      repo_pr:
        isOptional: true
        parameterType: NUMBER_INTEGER
      repo_url:
        defaultValue: https://github.com/instructlab/taxonomy.git
        isOptional: true
        parameterType: STRING
      storage_class_name:
        defaultValue: nfs-csi
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.9.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-artifact-to-pvc-op:
          pvcMount:
          - mountPath: /model
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc
        exec-artifact-to-pvc-op-2:
          pvcMount:
          - mountPath: /data
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc-2
        exec-list-models-in-directory-op:
          pvcMount:
          - mountPath: /output/model
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc-3
        exec-list-models-in-directory-op-2:
          pvcMount:
          - mountPath: /output/model
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc-3
        exec-pvc-to-artifact-op:
          pvcMount:
          - mountPath: /output/data
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc-3
        exec-pvc-to-model-op:
          pvcMount:
          - mountPath: /output/model
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc-3
        exec-run-mmlu-op:
          pvcMount:
          - mountPath: /output
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc-3
        exec-run-mt-bench-op:
          configMapAsEnv:
          - configMapName: kfp-model-server
            keyToEnv:
            - configMapKey: endpoint
              envVar: JUDGE_ENDPOINT
            - configMapKey: model
              envVar: JUDGE_NAME
          pvcMount:
          - mountPath: /output
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc-3
          secretAsEnv:
          - keyToEnv:
            - envVar: JUDGE_API_KEY
              secretKey: api_key
            secretName: judge-server
        exec-sdg-op:
          configMapAsEnv:
          - configMapName: kfp-model-server
            keyToEnv:
            - configMapKey: endpoint
              envVar: endpoint
            - configMapKey: model
              envVar: model
          secretAsEnv:
          - keyToEnv:
            - envVar: api_key
              secretKey: api_key
            secretName: kfp-model-server
